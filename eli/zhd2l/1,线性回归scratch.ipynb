{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 线性回归从零实现\n",
    "```\n",
    "训练步骤：\n",
    "1、初始化模型参数: w,b\n",
    "2、设置超参数: lr,num_epochs\n",
    "3、前向传播: 计算出y_hat\n",
    "4、计算出loss: （关于y_hat 与 y 的）squared_loss\n",
    "5、反向传播，计算梯度: （参数w、b关于loss函数的梯度）【注意：反向传播之前必须清除上一个 epoch 保存的梯度】\n",
    "6、使用梯度，更新参数: Optimization Algorithm （sgd、Adagrad、Adam等） 【注意：with torch.no_grad():】\n",
    "7、重复步骤3-6，num_epochs次。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.506145Z",
     "start_time": "2023-05-28T10:19:00.479853600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.524696200Z",
     "start_time": "2023-05-28T10:19:00.492631100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 设置plt可以显示中文字体，否则会显示乱码\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.525693200Z",
     "start_time": "2023-05-28T10:19:00.507145900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 2]), torch.Size([1000, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "生成加入噪声的测试数据\n",
    "生成y=Xw+b+噪声\n",
    "\"\"\"\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 高斯分布(正态分布)\n",
    "    # X = torch.rand((num_examples,len(w))) # 均匀分布\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "# 测试\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "features.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.798255100Z",
     "start_time": "2023-05-28T10:19:00.523696200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "查看测试数据散点图\n",
    "\"\"\"\n",
    "for i in range(len(true_w)):\n",
    "    plt.scatter(features[:, i], labels)\n",
    "    plt.xlabel(f\"feature-{i+1}\")\n",
    "    plt.ylabel(f\"label\")\n",
    "    plt.title(f\"第{i + 1}个特征features[:, {i}]和labels的散点图(w={true_w[i]})\", fontproperties=font)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.821119400Z",
     "start_time": "2023-05-28T10:19:00.800765200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 2]) torch.Size([500, 1])\n",
      "torch.Size([500, 2]) torch.Size([500, 1])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为 batch_size 的小批量.\n",
    "每个小批量包含一组特征和标签。\n",
    "\"\"\"\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        # 最后一个批次数据，应该特殊处理，防止最后一个batch 数据量过小。或者设置恰好被 数据总量 整除的 batch_size\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "# 测试\n",
    "for features_batch,labels_batch in data_iter(500,features,labels):\n",
    "    print(features_batch.shape,labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.841231500Z",
     "start_time": "2023-05-28T10:19:00.814796800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "初始化模型参数\n",
    "# 从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0\n",
    "\"\"\"\n",
    "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.856305300Z",
     "start_time": "2023-05-28T10:19:00.829625700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"线性回归模型\"\"\"\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.866945500Z",
     "start_time": "2023-05-28T10:19:00.846238Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"均方损失函数\"\"\"\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.874460100Z",
     "start_time": "2023-05-28T10:19:00.860947400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"小批量随机梯度下降 sgd算法\"\"\"\n",
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q：小批量随机梯度下降，为什么要除以batch_size?\n",
    "\n",
    "小批量随机梯度下降（mini-batch stochastic gradient descent）是一种在机器学习中常用的优化算法，用于训练模型的参数。它结合了随机梯度下降和批量梯度下降的优点。\n",
    "\n",
    "在小批量随机梯度下降中，训练数据被划分为多个小批量（mini-batches）。每个小批量包含一组样本和对应的标签。然后，对于每个小批量，计算梯度并更新模型的参数。\n",
    "\n",
    "为了理解为什么在更新参数时要除以`batch_size`，我们可以先回顾一下梯度下降的原理。\n",
    "\n",
    "梯度下降的目标是通过最小化损失函数来优化模型的参数。梯度表示损失函数对于参数的变化率，指示了参数更新的方向。在批量梯度下降中，每次更新时使用的是所有训练样本的梯度平均值。而在小批量随机梯度下降中，只使用一个小批量的样本来计算梯度。\n",
    "\n",
    "当我们计算梯度时，由于每个样本的损失函数梯度可能不同，所以在一个小批量中的样本的梯度也会有所不同。为了将这些梯度汇总为一个更新方向，我们需要对梯度值进行平均。除以`batch_size`就是为了对梯度进行平均。这样做的好处是：\n",
    "\n",
    "1. 加快计算速度：采用小批量样本计算梯度可以减少计算量，加快模型的训练速度。\n",
    "2. 控制更新步长：小批量样本的梯度平均值代表了整个训练数据集上的梯度方向，在更新模型参数时可以更好地控制步长，使优化过程更稳定。\n",
    "\n",
    "除以`batch_size`相当于对每个样本的梯度进行标准化，以保持梯度方向的相对稳定性，并且确保参数更新不会过于剧烈。这有助于模型更好地收敛到全局最优解或接近最优解的位置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:00.895273100Z",
     "start_time": "2023-05-28T10:19:00.875460600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"设置超参数\"\"\"\n",
    "lr = 0.03 # 学习率\n",
    "batch_size = 32 # 数据批量大小\n",
    "num_epochs = 10 # 训练轮数\n",
    "net = linreg # 线性回归 神经网络单元\n",
    "loss = squared_loss # 损失函数\n",
    "train_all_dataset_loss_list = [] # 用于保存历史loss均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:01.041777100Z",
     "start_time": "2023-05-28T10:19:00.891273500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # print(y.shape)\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_all_dataset_loss = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, 平均mean loss {float(train_all_dataset_loss.mean()):f}')\n",
    "        train_all_dataset_loss_list.append(train_all_dataset_loss.mean())\n",
    "\n",
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T10:19:01.122473600Z",
     "start_time": "2023-05-28T10:19:01.039687200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 画图-训练误差曲线\n",
    "train_all_dataset_loss_list_Y = train_all_dataset_loss_list\n",
    "train_all_dataset_loss_list_X = [i for i in range(len(train_all_dataset_loss_list_Y))]\n",
    "plt.plot(train_all_dataset_loss_list_X, train_all_dataset_loss_list_Y,marker='o')\n",
    "plt.title(\"训练误差曲线\", fontproperties=font)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
